{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb435d3f",
   "metadata": {},
   "source": [
    "<img src=\"../figs/holberton_logo.png\" alt=\"logo\" width=\"500\"/>\n",
    "\n",
    "# A Machine Learning Framework for Automated Albanian News Article Title Classification\n",
    "\n",
    "## 1. Executive Summary\n",
    "\n",
    "**Automated news article classification** is a technique to **classify text data into predefined categories**. This has many uses, such as improving search results, understanding topics, analyzing sentiments, and recommending content. In Albanian, despite the growth of digital content, there is a lack of large text datasets, which hinders the progress of natural language processing (NLP) research and applications.\n",
    "\n",
    "This work makes two main contributions:\n",
    "\n",
    "1. We introduce a new dataset with `9600` news article titles from various categories.\n",
    "2. We use this dataset to evaluate the performance of different machine learning algorithms for classifying topics.\n",
    "\n",
    "Our experiments show that recurrent neural networks (RNNs) perform better than simpler classifiers and ensemble methods for this task.\n",
    "\n",
    "### Applications\n",
    "\n",
    "Text classification is essential in natural language processing (NLP) with various practical uses. It helps in:\n",
    "\n",
    "- **Information Retrieval and Summarization**: Efficiently finding and summarizing relevant information from large datasets.\n",
    "- **News Aggregation**: Grouping news articles by topics for easier navigation.\n",
    "- **Customer Feedback Segmentation**: Analyzing and categorizing customer reviews and feedback for better insights.\n",
    "- **Content Personalization**: Tailoring content to individual user preferences to enhance user experience.\n",
    "\n",
    "### Challenges\n",
    "\n",
    "While text classification has many applications, it also faces several challenges, especially for the Albanian language:\n",
    "\n",
    "- **Limited Text Corpora**: Albanian has fewer large datasets available, hindering the development of NLP models.\n",
    "- **Ambiguity in News Articles**: Articles can cover multiple topics, making it difficult to assign a single category. For example, a sports article might also discuss cultural or social aspects.\n",
    "- **Grammatical and Lexical Variability**: The structure and style of Albanian text differ significantly from English, posing challenges for accurate classification.\n",
    "- **Resource Constraints**: Low-resource languages like Albanian lack the extensive training data required for sophisticated machine learning models to perform well\n",
    "\n",
    "## 2. Problem Definition\n",
    "\n",
    "### Problem Definition\n",
    "\n",
    "Let $\\mathcal{D}$ be a dataset with $N$ news article titles. Each title $x_i$ is linked to a category label $y_i$ such that $y_i \\in \\{1, 2, \\ldots, \\mathcal{K}\\}$, where $\\mathcal{K}$ is the total number of categories. Each news title $x_i$ consists of a sequence of words, represented as $x_i = (w_{i1}, w_{i2}, \\ldots, w_{iN_i})$, where $N_i$ is the number of words in title $x_i$. These words are the features used for classification.\n",
    "\n",
    "The goal is to use the training data to learn a function $f: \\mathcal{X} \\mapsto \\mathcal{Y}$, where $\\mathcal{X}$ is the set of news article titles, and $\\mathcal{Y}$ is the set of corresponding true labels. The function $f$ maps each title $x_i$ to its category $y_i$, aiming to accurately predict the category of new, unseen titles. The objective is to find the optimal function $f^*$ that minimizes a predefined loss function $L(f(x_i), y_i)$ over the entire dataset $\\mathcal{D}$, such that:\n",
    "\n",
    "$$\n",
    "f^* = \\underset{f}{\\text{argmin}} \\sum_{i=1}^N \\mathcal{L}(f(x_i), y_i)\n",
    "$$\n",
    "\n",
    "where $N$ is the total number of news article titles, and $\\mathcal{L}$ is the loss incurred by the model's prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9104667b",
   "metadata": {},
   "source": [
    "## 3. Proposed Methodology\n",
    "\n",
    "### Proposed Methodology\n",
    "\n",
    "Our approach consists of three main steps:\n",
    "\n",
    "1. **Dataset Creation**: We collect news article titles from different categories using web scraping.\n",
    "2. **Data Processing**: We convert the collected titles into numerical tokens using tokenization and also convert the labels into numerical values.\n",
    "3. **Model Training**: We build and train various classification models, including traditional methods, ensemble techniques, and deep learning models, to accurately classify new, unseen news article titles.\n",
    "\n",
    "Figure below provides an illustration of this process.\n",
    "\n",
    "<img src=\"../figs/5-portfolio/approach.png\" alt=\"logo\" width=\"800\"/>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0c5ea1",
   "metadata": {},
   "source": [
    "## 4. Technical Approach\n",
    "\n",
    "### 4.1 Data Preparation\n",
    "\n",
    "We **create a dataset by scraping the web for news article titles, covering six main categories: politics, economy, current affairs, sport, culture, and lifestyle**. To make our approach robust, we include categories that might overlap, like culture and lifestyle. \n",
    "\n",
    "After collecting the data, we preprocess it to ensure uniform formatting. This includes **removing punctuation and special symbols, converting text to lowercase, and discarding titles that are too short**. We balance the dataset to represent each category equally. A human supervisor reviews the data to ensure its quality and relevance.\n",
    "\n",
    "#### Key Steps\n",
    "\n",
    "1. **Import Libraries**: Import `requests`, `BeautifulSoup`, and `csv`.\n",
    "\n",
    "2. **Scrape Headlines Function**:\n",
    "    - **Purpose**: Scrape headlines from a given URL and add them to a list.\n",
    "    - **Process**:\n",
    "        - Send an HTTP GET request to the URL.\n",
    "        - Parse the HTML content using BeautifulSoup.\n",
    "        - Extract headlines (`<h3>` tags) and append them to the list, excluding the header and footer.\n",
    "        - Handle various HTTP and connection errors gracefully.\n",
    "\n",
    "3. **Main Function**:\n",
    "    - **Purpose**: Scrape multiple URLs and save all headlines to a CSV file.\n",
    "    - **Process**:\n",
    "        - Initialize an empty list to store headlines.\n",
    "        - For each base URL, scrape a specified number of pages.\n",
    "        - Call `scrape_headlines` for each page URL.\n",
    "        - Write the collected headlines to a CSV file with 'Headline' and 'Category' columns.\n",
    "\n",
    "4. **Example Usage**:\n",
    "    - Define a list of URLs to scrape.\n",
    "    - Set the number of pages to scrape per URL.\n",
    "    - Specify the output CSV file name.\n",
    "    - Call the `main` function with these parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eb452b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping URL: https://fjala.al/category/ekonomia\n",
      "Scraping URL: https://fjala.al/category/politics\n",
      "Scraping URL: https://fjala.al/category/aktualitet/kronike\n",
      "Scraping URL: https://fjala.al/category/sport\n",
      "Scraping URL: https://fjala.al/category/lifestyle/show-biz\n",
      "All headlines saved to all_headlines.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "def scrape_headlines(url, all_headlines):\n",
    "    \"\"\"\n",
    "    Scrapes headlines from the given URL and appends them to the all_headlines list.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the page to scrape.\n",
    "        all_headlines (list of tuples): The list to which scraped headlines will be appended.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Send the GET request using default headers\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raises HTTPError for bad responses (4xx or 5xx)\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        headlines = soup.find_all('h3')\n",
    "\n",
    "        # Append each headline to the list\n",
    "        for headline in headlines[1:-5]:       # Remove header and footer\n",
    "            text = headline.text.strip()\n",
    "            all_headlines.append((text, url))  # Save the category URL \n",
    "\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred for {url}: {http_err}\")\n",
    "    except requests.exceptions.ConnectionError as conn_err:\n",
    "        print(f\"Connection error occurred for {url}: {conn_err}\")\n",
    "    except requests.exceptions.Timeout as timeout_err:\n",
    "        print(f\"Timeout error occurred for {url}: {timeout_err}\")\n",
    "    except requests.exceptions.RequestException as req_err:\n",
    "        print(f\"An error occurred for {url}: {req_err}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred for {url}: {e}\")\n",
    "\n",
    "def main(urls, num_pages_per_url, output_csv):\n",
    "    \"\"\"\n",
    "    Scrapes multiple URLs and saves all headlines to a single CSV file.\n",
    "\n",
    "    Args:\n",
    "        urls (list of str): List of base URLs for scraping.\n",
    "        num_pages_per_url (int): Number of pages to scrape for each URL.\n",
    "        output_csv (str): The path to the CSV file where headlines will be saved.\n",
    "    \"\"\"\n",
    "    all_headlines = []\n",
    "\n",
    "    for base_url in urls:\n",
    "        print(f\"Scraping URL: {base_url}\")  # Debugging: Print which URL is being scraped\n",
    "        for i in range(1, num_pages_per_url + 1):\n",
    "            url = f'{base_url}/page/{i}/'\n",
    "            # print(f\"Scraping page: {url}\")  # Debugging: Print page URL\n",
    "            scrape_headlines(url, all_headlines)\n",
    "\n",
    "    # Write all headlines to a single CSV file\n",
    "    with open(output_csv, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Headline', 'Category'])  # Write header row\n",
    "        writer.writerows(all_headlines)            # Write all headlines\n",
    "\n",
    "    print(f\"All headlines saved to {output_csv}\")\n",
    "\n",
    "# Provide here the urls to scrape from\n",
    "urls = [\n",
    "    'https://fjala.al/category/ekonomia',\n",
    "    'https://fjala.al/category/politics',\n",
    "    'https://fjala.al/category/aktualitet/kronike',\n",
    "    'https://fjala.al/category/sport',\n",
    "    'https://fjala.al/category/lifestyle/show-biz'\n",
    "]\n",
    "\n",
    "num_pages_per_url = 5\n",
    "output_csv = 'all_headlines.csv'\n",
    "main(urls, num_pages_per_url, output_csv)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
